An example of Transformer NMT training
######################################

1. Prepare the corpus.
======================

Firstly, prepare the parallel corpus.
In this example, ASPEC-JE is used.

.. code:: bash

   % export ASPEC_JE=/path/to/ASPEC-JE # replace to the collect path.
   % ./prepare-aspec.sh

:code:`prepare-aspec.sh` processes the following typical NMT pre-processing steps automatically:

#. Install the required tools.
#. Sample the training set.
#. Extract the parallel sentences from the ASPEC file format.
#. Remove ASPEC-specific noise.
#. Tokenize Japanese sentences by KyTea.
#. Tokenize English sentences by Moses.
#. Clean the parallel corpus based on sentence lengths.
#. Learn Byte-Pair-Encoding (BPE) on the training set for sub-word segmentation.
#. Apply BPE for the data-set.

2. Pre-process the dataset.
===========================

Convert the data-set to the binarized fairseq format by :code:`fairseq-preprocess`.

.. code:: bash

   % fairseq-preprocess \
          --source-lang ja --target-lang en \
          --trainpref aspec_ja_en/train \
          --validpref aspec_ja_en/dev \
          --testpref aspec_ja_en/devtest \
          --destdir binarized \
          --workers 8

3. Training a Transformer NMT model.
====================================

In fairseq, there are two training commands:

* :code:`fairseq-train` is used for single model training.
* :code:`fairseq-hydra-train` is used for multiple models training (e.g., hyper-parameter search) or loading the hierarchical YAML-based configurations.

:code:`fairseq-hydra-train` is richer command than :code:`fairseq-train`, but requires an understanding of `hydra <https://hydra.cc>`_.

Training a model w/ :code:`fairseq-train`
-----------------------------------------

.. code:: bash

   % export CUDA_VISIBLE_DEVICES=0
   % fairseq-train \
          binarized/ \
          --task translation \
          --arch transformer --share-decoder-input-output-embed --activation-fn relu \
          --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \
          --lr 7e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 --warmup-init-lr 1e-7 \
          --weight-decay 0.0001 \
          --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \
          --max-tokens 6000 \
          --save-dir checkpoints/ \
          --max-epoch 30 --no-epoch-checkpoints \
          --fp16

If you want to use Tensorboard for logging training statistics, add :code:`--tensorboard-logdir DIR`.
You can also use Weights and Biases (a rich logging service) with :code:`--wandb-project NAME`.

Training a model w/ :code:`fairseq-hydra-train`
-----------------------------------------------

.. code:: bash

   % export CUDA_VISIBLE_DEVICES=0
   % fairseq-hydra-train \
          --config-dir ./ --config-name transformer \
          checkpoint.save_dir=checkpoints/ \
          +task.data=$(pwd)/binarized/

4. Generate the translations
============================

In this example, translations are generated by the checkpoint that achieves minimum validation loss.
If you want to average checkpoints, use :code:`scripts/average_checkpoints.py` in a fairseq repository.

.. code:: bash

   % fairseq-generate \
      binarized/ \
      --gen-subset test \
      --path checkpoints/checkpoint_best.pt \
      --max-len-a 1 --max-len-b 50 \
      --beam 5 --lenpen 1.0 \
      --nbest 1 \
      --post-process subword_nmt \
      --results-path results/ \
      --fp16

Then, extract and de-tokenize translation hypotheses, and compute BLEU and RIBES scores.

.. code:: bash

   % grep "^H-" results/generate-test.txt | sort -V | cut -f3 > results/hypothesis-test.txt
   % ./detokenizer.sh en < results/hypothesis-test.txt > results/hypothesis-test.detok.txt
   % ./evaluate-aspec.sh en /path/to/ASPEC-JE/devtest/devtest.txt < results/hypothesis-test.detok.txt

:code:`evaluate-aspec.sh` generates the reference and computes BLEU and RIBES scores based on `WAT Automatic Evaluation Procedures <http://lotus.kuee.kyoto-u.ac.jp/WAT/evaluation/automatic_evaluation_systems/automaticEvaluationEN.html>`_.

5. Interactive Translation
==========================

It can be translate interactively with :code:`fairseq-interactive`.
In the default, it inputs from stdin.

.. code:: bash

   % fairseq-interactive \
      binarized/ \
      --path checkpoints/checkpoint_best.pt \
      --max-len-a 1 --max-len-b 50 \
      --beam 5 --lenpen 1.0 \
      --nbest 1 \
      --post-process subword_nmt \
      --fp16
